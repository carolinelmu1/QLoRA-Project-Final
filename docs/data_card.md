# Data Card: Stanford Alpaca Dataset

## Dataset Summary

**Name:** Stanford Alpaca  
**Version:** 1.0  
**Release Date:** March 2023  
**Organization:** Stanford CRFM (Center for Research on Foundation Models)  
**License:** CC BY NC 4.0 (Non-commercial use only)

### Description

The Stanford Alpaca dataset consists of 52,000 instruction-following demonstrations generated using OpenAI's text-davinci-003 (GPT-3.5) model via the Self-Instruct methodology. The dataset is designed for fine-tuning language models to follow natural language instructions.

**Key Characteristics:**
- 52,000 instruction-response pairs
- Diverse tasks covering multiple domains
- Generated from 175 human-written seed instructions
- High-quality responses curated through systematic prompting

## Dataset Structure

### Data Format

Each example contains three fields:

```json
{
  "instruction": "A natural language instruction",
  "input": "Optional context or input (empty string if not needed)",
  "output": "The expected response or completion"
}
```

### Example

```json
{
  "instruction": "Identify the odd one out from the following list of fruits.",
  "input": "Apple, Banana, Carrot, Orange",
  "output": "Carrot is the odd one out, as it is a vegetable while the rest are fruits."
}
```

### Task Distribution

The dataset covers diverse instruction types:

| Task Category | Approximate % | Examples |
|--------------|--------------|----------|
| Open QA | ~25% | "What is the capital of France?" |
| Closed QA | ~20% | "Is the following statement true or false?" |
| Classification | ~15% | "Categorize the following items" |
| Generation | ~15% | "Write a short story about..." |
| Rewriting | ~10% | "Paraphrase the following text" |
| Summarization | ~10% | "Summarize this article" |
| Other | ~5% | Math, code, reasoning |

## Dataset Creation

### Source Data

**Seed Instructions:** 175 human-written instructions covering diverse tasks

**Generation Method:** Self-Instruct (Wang et al., 2022)
1. Sample seed instructions
2. Generate new instructions using GPT-3.5
3. Filter for quality and diversity
4. Generate input-output pairs

**Model Used:** OpenAI text-davinci-003 (GPT-3.5-turbo)

### Data Collection

**Timeline:** January - March 2023  
**Cost:** ~$500 USD (OpenAI API costs)  
**Human Involvement:**
- Initial seed instruction authoring (175 examples)
- Quality review and filtering
- No manual annotation of generated pairs

### Data Processing

1. **Generation:** Systematic prompting of GPT-3.5 with seed instructions
2. **Filtering:** Remove duplicates, low-quality, or inappropriate responses
3. **Validation:** Automated checks for format consistency
4. **Release:** Publicly released on GitHub and HuggingFace

## Subset Used in This Project

**Size:** 1,000 samples (randomly selected)  
**Purpose:** Diagnostic experiments with fast iteration  
**Split:** 90% train (900 samples), 10% eval (100 samples)  
**Sampling:** Random selection with seed=42 for reproducibility

**Preprocessing for This Project:**
- Tokenized using GPT-2 tokenizer
- Max sequence length: 512 tokens
- Formatted as: `### Instruction:\n{instruction}\n\n### Response:\n{response}`
- Padding: Right-padded to max length

## Known Limitations

### Data Quality

1. **Synthetic Generation:**
   - All outputs generated by GPT-3.5 (not human-written)
   - May contain factual errors or inconsistencies
   - Reflects GPT-3.5 biases and limitations

2. **Instruction Diversity:**
   - Limited by 175 seed instructions
   - May not cover all real-world use cases
   - Skewed toward common NLP tasks

3. **Language:**
   - English only
   - May not generalize to multilingual settings

### Bias and Fairness

**Inherited Biases:**
- GPT-3.5 training data biases (unknown but likely similar to GPT-2/GPT-3)
- Potential demographic skew in seed instruction authorship
- Cultural biases toward Western/English-speaking contexts

**Known Issues:**
- Gender stereotypes in role-based examples
- Cultural assumptions (e.g., holidays, geography)
- Potential toxic content despite filtering

**Bias Analysis:** Limited formal bias analysis conducted on this dataset

## Ethical Considerations

### Intended Use

✅ **Appropriate Uses:**
- Academic research on instruction-following
- Educational demonstrations of fine-tuning techniques
- Non-commercial applications and experimentation

❌ **Inappropriate Uses:**
- Commercial deployment (violates CC BY NC 4.0 license)
- High-stakes applications without validation
- Generating content claimed as human-authored

### Privacy

- No personally identifiable information (PII) intentionally included
- Generated synthetically (not scraped from user data)
- However, GPT-3.5 may have memorized PII from training data

### Terms of Use

**License:** CC BY NC 4.0  
**Requirements:**
- Attribution to Stanford CRFM
- Non-commercial use only
- Share-alike (derivatives must use same license)

**Commercial Use:** Prohibited without explicit permission

## Access

**HuggingFace:** `tatsu-lab/alpaca`  
**GitHub:** [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)  
**Download Size:** ~24 MB (JSON format)

## Citation

If you use the Alpaca dataset, please cite:

```bibtex
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
```

**Self-Instruct Paper:**
```bibtex
@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
```

## Maintenance

**Maintainer:** Stanford CRFM  
**Last Updated:** March 2023  
**Version History:** 1.0 (initial release)

**Known Issues:** See GitHub issues page for community-reported problems

## Data Card Contact

For questions about this data card or the dataset subset used in this project:

**Project Author:** Caroline Ellis  
**Email:** [your_email@vanderbilt.edu]  
**GitHub:** [https://github.com/[YOUR_USERNAME]/QLoRA-Project]

For questions about the original Alpaca dataset:

**Stanford CRFM:** [https://crfm.stanford.edu/](https://crfm.stanford.edu/)  
**GitHub Issues:** [https://github.com/tatsu-lab/stanford_alpaca/issues](https://github.com/tatsu-lab/stanford_alpaca/issues)