{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Diagnostic Analysis - Part 3: Comprehensive Diagnostic Analysis\n",
    "\n",
    "## Objective\n",
    "Test the three core hypotheses and provide diagnostic insights into QLoRA's performance characteristics.\n",
    "\n",
    "## Hypotheses to Test\n",
    "1. **Quantization Impact**: If weight similarity (cosine sim) > 0.95, QLoRA should always be preferred\n",
    "2. **Layer Sensitivity**: Which transformer layers are most sensitive to quantization?\n",
    "3. **Rank Threshold**: What is the minimum rank r* that preserves quality?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q transformers datasets accelerate peft bitsandbytes matplotlib seaborn pandas numpy scikit-learn scipy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model_utils import (\n",
    "    load_base_model_16bit,\n",
    "    load_base_model_4bit,\n",
    "    setup_lora_16bit,\n",
    "    setup_lora_4bit,\n",
    "    clear_memory\n",
    ")\n",
    "\n",
    "from evaluation import (\n",
    "    evaluate_token_match,\n",
    "    evaluate_embedding_similarity,\n",
    "    compare_weight_matrices,\n",
    "    comprehensive_evaluation\n",
    ")\n",
    "\n",
    "from visualization import (\n",
    "    plot_rank_threshold_analysis,\n",
    "    plot_weight_similarity_matrix,\n",
    "    print_diagnostic_summary\n",
    ")\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline LoRA results\n",
    "with open('../results_baseline_lora/baseline_results.pkl', 'rb') as f:\n",
    "    baseline_results = pickle.load(f)\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "\n",
    "# Load QLoRA results\n",
    "with open('../results_qlora/qlora_results.pkl', 'rb') as f:\n",
    "    qlora_results = pickle.load(f)\n",
    "qlora_df = pd.DataFrame(qlora_results)\n",
    "\n",
    "print(f\"‚úì Loaded {len(baseline_results)} baseline results\")\n",
    "print(f\"‚úì Loaded {len(qlora_results)} QLoRA results\")\n",
    "\n",
    "# Combine for analysis\n",
    "combined_df = pd.concat([baseline_df, qlora_df], ignore_index=True)\n",
    "print(f\"\\nTotal experiments: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hypothesis 1: Quantization Impact (Weight Similarity Analysis)\n",
    "\n",
    "**Hypothesis:** If cosine similarity between LoRA and QLoRA adapter weights > 0.95, then QLoRA should always be preferred.\n",
    "\n",
    "### 3.1 Load Trained Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll compare rank 8 models (good middle ground)\n",
    "COMPARISON_RANK = 8\n",
    "MODEL_NAME = \"gpt2-medium\"\n",
    "\n",
    "print(f\"Loading models with rank={COMPARISON_RANK} for comparison...\")\n",
    "\n",
    "# Load LoRA model\n",
    "print(\"\\n1. Loading LoRA (16-bit) model...\")\n",
    "lora_model_path = f\"../results_baseline_lora/16bit_r{COMPARISON_RANK}/final_model\"\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    base_model_16, tokenizer = load_base_model_16bit(MODEL_NAME)\n",
    "    lora_model = PeftModel.from_pretrained(base_model_16, lora_model_path)\n",
    "    print(\"‚úì LoRA model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load LoRA model: {e}\")\n",
    "    print(\"Note: Models must be trained and saved first\")\n",
    "    lora_model = None\n",
    "\n",
    "# Load QLoRA model\n",
    "print(\"\\n2. Loading QLoRA (4-bit) model...\")\n",
    "qlora_model_path = f\"../results_qlora/4bit_r{COMPARISON_RANK}/final_model\"\n",
    "try:\n",
    "    base_model_4bit, _ = load_base_model_4bit(MODEL_NAME)\n",
    "    qlora_model = PeftModel.from_pretrained(base_model_4bit, qlora_model_path)\n",
    "    print(\"‚úì QLoRA model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load QLoRA model: {e}\")\n",
    "    qlora_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare Adapter Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lora_model and qlora_model:\n",
    "    # Compare weights across multiple layers\n",
    "    layers_to_compare = [0, 12, 23]  # First, middle, last layers\n",
    "    \n",
    "    weight_similarities = {}\n",
    "    \n",
    "    for layer_idx in layers_to_compare:\n",
    "        layer_name = f\"transformer.h.{layer_idx}.attn.c_attn\"\n",
    "        \n",
    "        print(f\"\\nComparing layer {layer_idx}...\")\n",
    "        result = compare_weight_matrices(lora_model, qlora_model, layer_name)\n",
    "        \n",
    "        if result:\n",
    "            weight_similarities[f\"Layer {layer_idx}\"] = result['cosine_similarity']\n",
    "            print(f\"  Cosine similarity: {result['cosine_similarity']:.4f}\")\n",
    "            print(f\"  L2 distance: {result['l2_distance']:.4f}\")\n",
    "            print(f\"  Relative difference: {result['relative_difference']:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WEIGHT SIMILARITY SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    mean_similarity = np.mean(list(weight_similarities.values()))\n",
    "    print(f\"Mean cosine similarity: {mean_similarity:.4f}\")\n",
    "    print(f\"Threshold (0.95): {'‚úì MET' if mean_similarity >= 0.95 else '‚úó NOT MET'}\")\n",
    "    \n",
    "    # Hypothesis verdict\n",
    "    if mean_similarity >= 0.95:\n",
    "        print(\"\\n‚úÖ HYPOTHESIS SUPPORTED: QLoRA preserves weight information\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  HYPOTHESIS CHALLENGED: Significant weight divergence detected\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping weight comparison (models not loaded)\")\n",
    "    weight_similarities = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualize Weight Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weight_similarities:\n",
    "    plot_weight_similarity_matrix(\n",
    "        weight_similarities,\n",
    "        save_path='../results/figures/weight_similarity_matrix.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis 2: Rank Threshold Analysis\n",
    "\n",
    "**Question:** What is the minimum rank r* that preserves acceptable quality?\n",
    "\n",
    "### 4.1 Analyze Performance vs Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss vs rank\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# LoRA\n",
    "lora_data = baseline_df.sort_values('rank')\n",
    "ax.plot(lora_data['rank'], lora_data['training_loss'], \n",
    "        marker='o', markersize=10, linewidth=2.5,\n",
    "        label='LoRA (16-bit)', color='#3498db', alpha=0.8)\n",
    "\n",
    "# QLoRA\n",
    "qlora_data = qlora_df.sort_values('rank')\n",
    "ax.plot(qlora_data['rank'], qlora_data['training_loss'],\n",
    "        marker='s', markersize=10, linewidth=2.5,\n",
    "        label='QLoRA (4-bit)', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('LoRA Rank (r)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Rank Threshold Analysis: Loss vs Rank', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xticks([2, 4, 8, 16])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/rank_threshold_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify rank threshold\n",
    "print(\"\\nüìä RANK THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "for rank in [2, 4, 8, 16]:\n",
    "    lora_loss = baseline_df[baseline_df['rank'] == rank]['training_loss'].values[0]\n",
    "    qlora_loss = qlora_df[qlora_df['rank'] == rank]['training_loss'].values[0]\n",
    "    diff = abs(lora_loss - qlora_loss)\n",
    "    print(f\"Rank {rank:2d}: LoRA={lora_loss:.4f}, QLoRA={qlora_loss:.4f}, Diff={diff:.4f}\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION:\")\n",
    "print(\"[TODO: Fill in after running - e.g., 'Significant degradation at r=2, stable at r‚â•4']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Performance Degradation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative performance degradation\n",
    "degradation_analysis = []\n",
    "\n",
    "for rank in [2, 4, 8, 16]:\n",
    "    lora_loss = baseline_df[baseline_df['rank'] == rank]['training_loss'].values[0]\n",
    "    qlora_loss = qlora_df[qlora_df['rank'] == rank]['training_loss'].values[0]\n",
    "    \n",
    "    degradation_pct = ((qlora_loss - lora_loss) / lora_loss) * 100\n",
    "    \n",
    "    degradation_analysis.append({\n",
    "        'rank': rank,\n",
    "        'lora_loss': lora_loss,\n",
    "        'qlora_loss': qlora_loss,\n",
    "        'degradation_%': degradation_pct,\n",
    "        'acceptable': 'YES' if abs(degradation_pct) < 5 else 'NO'\n",
    "    })\n",
    "\n",
    "degradation_df = pd.DataFrame(degradation_analysis)\n",
    "\n",
    "print(\"\\nüîç DEGRADATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Threshold: <5% degradation considered acceptable\\n\")\n",
    "display(degradation_df)\n",
    "\n",
    "# Identify minimum viable rank\n",
    "acceptable_ranks = degradation_df[degradation_df['acceptable'] == 'YES']['rank'].tolist()\n",
    "if acceptable_ranks:\n",
    "    min_rank = min(acceptable_ranks)\n",
    "    print(f\"\\n‚ú® Minimum viable rank: r* = {min_rank}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No ranks meet acceptability threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hypothesis 3: Layer Sensitivity Analysis\n",
    "\n",
    "**Question:** Which transformer weight matrices are most sensitive to quantization?\n",
    "\n",
    "### 5.1 Memory vs Performance Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot: memory vs performance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# LoRA\n",
    "ax.scatter(baseline_df['peak_memory_mb'], baseline_df['training_loss'],\n",
    "           s=200, alpha=0.6, color='#3498db', label='LoRA (16-bit)', edgecolors='black')\n",
    "\n",
    "# QLoRA\n",
    "ax.scatter(qlora_df['peak_memory_mb'], qlora_df['training_loss'],\n",
    "           s=200, alpha=0.6, color='#e74c3c', label='QLoRA (4-bit)', \n",
    "           marker='s', edgecolors='black')\n",
    "\n",
    "# Annotate ranks\n",
    "for _, row in baseline_df.iterrows():\n",
    "    ax.annotate(f\"r={int(row['rank'])}\", \n",
    "                (row['peak_memory_mb'], row['training_loss']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "for _, row in qlora_df.iterrows():\n",
    "    ax.annotate(f\"r={int(row['rank'])}\", \n",
    "                (row['peak_memory_mb'], row['training_loss']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Peak GPU Memory (MB)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Memory vs Performance Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/memory_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Pareto Frontier: Lower-left is optimal (low memory, low loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Failure Mode Documentation\n",
    "\n",
    "### 6.1 Identify Failure Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ö†Ô∏è  DOCUMENTED FAILURE MODES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Failure Mode 1: Insufficient Rank\n",
    "r2_degradation = degradation_df[degradation_df['rank'] == 2]['degradation_%'].values[0]\n",
    "if abs(r2_degradation) > 5:\n",
    "    print(\"\\n1. INSUFFICIENT RANK (r < r*)\")\n",
    "    print(f\"   Symptom: At r=2, degradation = {r2_degradation:.2f}%\")\n",
    "    print(\"   Cause: Low-rank bottleneck cannot capture task complexity\")\n",
    "    print(f\"   Mitigation: Use rank ‚â• {min_rank if acceptable_ranks else 4}\")\n",
    "\n",
    "# Failure Mode 2: Weight Divergence\n",
    "if weight_similarities and mean_similarity < 0.95:\n",
    "    print(\"\\n2. WEIGHT DIVERGENCE\")\n",
    "    print(f\"   Symptom: Cosine similarity = {mean_similarity:.4f} < 0.95\")\n",
    "    print(\"   Cause: Quantization noise exceeds low-rank capacity\")\n",
    "    print(\"   Mitigation: Increase rank or use 8-bit quantization\")\n",
    "\n",
    "# Failure Mode 3: [Add more based on observations]\n",
    "print(\"\\n3. [TODO: Document additional failure modes observed in experiments]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Diagnostic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final diagnostic summary\n",
    "diagnostic_summary = {\n",
    "    'lora_memory_mb': baseline_df[baseline_df['rank'] == 8]['peak_memory_mb'].values[0],\n",
    "    'qlora_memory_mb': qlora_df[qlora_df['rank'] == 8]['peak_memory_mb'].values[0],\n",
    "    'mean_cosine_similarity': mean_similarity if weight_similarities else None,\n",
    "    'mean_token_match': None,  # Would need evaluation dataset\n",
    "    'lora_time_per_step': baseline_df[baseline_df['rank'] == 8]['time_per_step'].values[0],\n",
    "    'qlora_time_per_step': qlora_df[qlora_df['rank'] == 8]['time_per_step'].values[0],\n",
    "}\n",
    "\n",
    "print_diagnostic_summary(diagnostic_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Recommendations\n",
    "\n",
    "### 8.1 When to Use QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ USE QLORA WHEN:\")\n",
    "print(f\"  ‚Ä¢ Rank r ‚â• {min_rank if acceptable_ranks else 4}\")\n",
    "print(\"  ‚Ä¢ GPU memory is constrained\")\n",
    "print(\"  ‚Ä¢ Training on instruction-following tasks\")\n",
    "if weight_similarities and mean_similarity >= 0.95:\n",
    "    print(f\"  ‚Ä¢ Weight similarity confirmed (cosine sim = {mean_similarity:.4f})\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  USE STANDARD LORA WHEN:\")\n",
    "print(\"  ‚Ä¢ Very low rank required (r < 4)\")\n",
    "print(\"  ‚Ä¢ Maximum precision needed (e.g., mathematical reasoning)\")\n",
    "print(\"  ‚Ä¢ GPU memory not a constraint\")\n",
    "\n",
    "print(\"\\nüìä OPTIMAL CONFIGURATION:\")\n",
    "memory_reduction = ((diagnostic_summary['lora_memory_mb'] - diagnostic_summary['qlora_memory_mb']) / \n",
    "                   diagnostic_summary['lora_memory_mb']) * 100\n",
    "print(f\"  ‚Ä¢ Rank: r = 8 (balanced performance/efficiency)\")\n",
    "print(f\"  ‚Ä¢ Memory savings: {memory_reduction:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Performance: Comparable to 16-bit LoRA\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results for README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary for README\n",
    "readme_results = {\n",
    "    'memory_comparison': degradation_df[['rank', 'lora_loss', 'qlora_loss', 'degradation_%']].to_dict('records'),\n",
    "    'weight_similarities': weight_similarities,\n",
    "    'optimal_rank': min_rank if acceptable_ranks else 4,\n",
    "    'memory_reduction': memory_reduction,\n",
    "    'diagnostic_summary': diagnostic_summary\n",
    "}\n",
    "\n",
    "# Save for reference\n",
    "with open('../results/tables/diagnostic_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(readme_results, f)\n",
    "\n",
    "print(\"\\n‚úÖ DIAGNOSTIC ANALYSIS COMPLETE!\")\n",
    "print(\"\\nüìã TODO: Update README.md with these results:\")\n",
    "print(\"  1. Fill memory comparison table\")\n",
    "print(\"  2. Add weight similarity findings\")\n",
    "print(\"  3. Document rank threshold (r* = ...)\")\n",
    "print(\"  4. Add failure mode descriptions\")\n",
    "print(\"  5. Complete critical analysis section\")\n",
    "print(\"\\nüéâ Ready for presentation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate All Remaining Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all plots are saved\n",
    "print(\"\\nüìä Generating final plots...\\n\")\n",
    "\n",
    "# Already created:\n",
    "# - memory_comparison.png\n",
    "# - training_efficiency.png\n",
    "# - rank_threshold_plot.png\n",
    "# - weight_similarity_matrix.png\n",
    "# - memory_vs_performance.png\n",
    "\n",
    "print(\"‚úì All plots generated in ../results/figures/\")\n",
    "print(\"\\nPlot files:\")\n",
    "import glob\n",
    "for plot in glob.glob('../results/figures/*.png'):\n",
    "    print(f\"  - {plot}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
