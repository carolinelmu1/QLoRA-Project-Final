{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Diagnostic Analysis - Part 2: QLoRA (4-bit) Implementation\n",
        "\n",
        "## Objective\n",
        "Implement QLoRA with 4-bit NF4 quantization and compare against the 16-bit LoRA baseline from Part 1.\n",
        "\n",
        "## Key Questions\n",
        "1. How much memory does 4-bit quantization save compared to 16-bit?\n",
        "2. Does QLoRA preserve performance (cosine similarity > 0.95)?\n",
        "3. What is the optimal rank for QLoRA?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers datasets accelerate peft bitsandbytes matplotlib seaborn pandas numpy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utilities\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "\n",
        "from model_utils import (\n",
        "    load_base_model_4bit,\n",
        "    setup_lora_4bit,\n",
        "    get_model_memory_usage,\n",
        "    print_model_architecture,\n",
        "    clear_memory\n",
        ")\n",
        "\n",
        "from training import (\n",
        "    prepare_alpaca_dataset,\n",
        "    train_model,\n",
        "    run_experiment\n",
        ")\n",
        "\n",
        "from visualization import (\n",
        "    plot_memory_comparison,\n",
        "    create_results_table,\n",
        "    print_diagnostic_summary\n",
        ")\n",
        "\n",
        "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experimental configuration\n",
        "MODEL_NAME = \"gpt2-medium\"  # 355M parameters\n",
        "NUM_SAMPLES = 1000  # Match baseline\n",
        "MAX_STEPS = 200\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "# Ranks to test (match baseline)\n",
        "RANKS_TO_TEST = [2, 4, 8, 16]\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"./results_qlora\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Quantization: 4-bit NF4\")\n",
        "print(f\"  Training samples: {NUM_SAMPLES}\")\n",
        "print(f\"  Max steps: {MAX_STEPS}\")\n",
        "print(f\"  Ranks to test: {RANKS_TO_TEST}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Baseline Results for Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline LoRA results\n",
        "try:\n",
        "    with open('../results_baseline_lora/baseline_results.pkl', 'rb') as f:\n",
        "        baseline_results = pickle.load(f)\n",
        "    print(f\"‚úì Loaded {len(baseline_results)} baseline results\")\n",
        "    baseline_df = pd.DataFrame(baseline_results)\n",
        "    print(\"\\nBaseline Summary:\")\n",
        "    display(baseline_df[['rank', 'peak_memory_mb', 'time_per_step', 'training_loss']])\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è  Baseline results not found. Run 01_baseline_lora.ipynb first.\")\n",
        "    baseline_results = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run QLoRA Experiments\n",
        "\n",
        "Train QLoRA (4-bit quantized base + high-precision adapters) with different ranks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results\n",
        "qlora_results_list = []\n",
        "\n",
        "for rank in RANKS_TO_TEST:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running QLoRA (4-bit) with rank r={rank}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result, model, tokenizer = run_experiment(\n",
        "            model_name=MODEL_NAME,\n",
        "            quantization=\"4bit\",\n",
        "            rank=rank,\n",
        "            num_samples=NUM_SAMPLES,\n",
        "            max_steps=MAX_STEPS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            output_dir=OUTPUT_DIR\n",
        "        )\n",
        "        \n",
        "        qlora_results_list.append(result)\n",
        "        \n",
        "        # Clean up\n",
        "        del model\n",
        "        del tokenizer\n",
        "        clear_memory()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error with rank {rank}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n‚úì All QLoRA experiments complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results Analysis\n",
        "\n",
        "### 5.1 Create Results Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create QLoRA results table\n",
        "qlora_df = create_results_table(\n",
        "    qlora_results_list,\n",
        "    save_path=f\"{OUTPUT_DIR}/qlora_results.csv\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä QLORA RESULTS\")\n",
        "print(\"=\"*80)\n",
        "display(qlora_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Compare LoRA vs QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if baseline_results:\n",
        "    # Combine results\n",
        "    combined_df = pd.concat([baseline_df, qlora_df], ignore_index=True)\n",
        "    \n",
        "    # Calculate memory reduction\n",
        "    comparison = pd.DataFrame()\n",
        "    for rank in RANKS_TO_TEST:\n",
        "        lora_mem = baseline_df[baseline_df['rank'] == rank]['peak_memory_mb'].values[0]\n",
        "        qlora_mem = qlora_df[qlora_df['rank'] == rank]['peak_memory_mb'].values[0]\n",
        "        reduction = ((lora_mem - qlora_mem) / lora_mem) * 100\n",
        "        \n",
        "        comparison = pd.concat([comparison, pd.DataFrame({\n",
        "            'rank': [rank],\n",
        "            'lora_memory_mb': [lora_mem],\n",
        "            'qlora_memory_mb': [qlora_mem],\n",
        "            'memory_reduction_%': [reduction]\n",
        "        })], ignore_index=True)\n",
        "    \n",
        "    print(\"\\nüîã MEMORY COMPARISON: LoRA vs QLoRA\")\n",
        "    print(\"=\"*80)\n",
        "    display(comparison)\n",
        "    \n",
        "    print(f\"\\n‚ú® Average memory reduction: {comparison['memory_reduction_%'].mean():.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Visualize Memory Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if baseline_results:\n",
        "    # Plot memory comparison\n",
        "    plot_memory_comparison(\n",
        "        combined_df,\n",
        "        save_path=f\"../results/figures/memory_comparison.png\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Training Efficiency Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if baseline_results:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Time per step\n",
        "    ax1.bar(baseline_df['rank'] - 0.2, baseline_df['time_per_step'], 0.4, \n",
        "            label='LoRA (16-bit)', color='#3498db', alpha=0.8)\n",
        "    ax1.bar(qlora_df['rank'] + 0.2, qlora_df['time_per_step'], 0.4,\n",
        "            label='QLoRA (4-bit)', color='#e74c3c', alpha=0.8)\n",
        "    ax1.set_xlabel('Rank', fontweight='bold')\n",
        "    ax1.set_ylabel('Time per Step (s)', fontweight='bold')\n",
        "    ax1.set_title('Training Speed Comparison')\n",
        "    ax1.legend()\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Training loss\n",
        "    ax2.plot(baseline_df['rank'], baseline_df['training_loss'], \n",
        "             marker='o', linewidth=2, label='LoRA (16-bit)', color='#3498db')\n",
        "    ax2.plot(qlora_df['rank'], qlora_df['training_loss'],\n",
        "             marker='s', linewidth=2, label='QLoRA (4-bit)', color='#e74c3c')\n",
        "    ax2.set_xlabel('Rank', fontweight='bold')\n",
        "    ax2.set_ylabel('Training Loss', fontweight='bold')\n",
        "    ax2.set_title('Training Loss Comparison')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../results/figures/training_efficiency.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Findings\n",
        "\n",
        "### TODO: Fill in after running experiments\n",
        "\n",
        "**Memory Reduction:**\n",
        "- Average reduction: [TODO: FILL]%\n",
        "- Rank 8: LoRA [TODO] MB ‚Üí QLoRA [TODO] MB\n",
        "\n",
        "**Performance:**\n",
        "- Training loss comparable: [YES/NO]\n",
        "- Time per step: [FASTER/SLOWER/SIMILAR]\n",
        "\n",
        "**Observations:**\n",
        "- [TODO: Document trends]\n",
        "- [TODO: Note any unexpected behavior]\n",
        "\n",
        "---\n",
        "\n",
        "**Next Steps:**\n",
        "- Proceed to Part 3: Diagnostic analysis (weight similarity, hypothesis testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save QLoRA results\n",
        "with open(f\"{OUTPUT_DIR}/qlora_results.pkl\", 'wb') as f:\n",
        "    pickle.dump(qlora_results_list, f)\n",
        "\n",
        "# Save comparison\n",
        "if baseline_results:\n",
        "    comparison.to_csv('../results/tables/memory_comparison.csv', index=False)\n",
        "\n",
        "print(f\"‚úì Results saved to {OUTPUT_DIR}/\")\n",
        "print(\"\\nüéâ QLoRA experiments complete!\")\n",
        "print(\"üìù Proceed to notebook 03_diagnostic_analysis.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
